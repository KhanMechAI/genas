{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 32 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "\n",
    "     # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "my_nn = Net()\n",
    "my_nn.to(device)\n",
    "\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST('mnist_test', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "writer.add_image('images', grid, 0)\n",
    "writer.add_graph(my_nn, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [trainset.classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels, classes):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "writer = SummaryWriter()\n",
    "running_loss = 0.0\n",
    "my_nn = Net()\n",
    "my_nn.to(device)\n",
    "optimizer = optim.Adam(my_nn.parameters(), lr=0.001,)   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "milestones = [k for k in range(0, num_epochs*len(trainloader), 50)]\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.95)\n",
    "\n",
    "start_t = default_timer()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs-1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = [d.to(device) for d in data]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = my_nn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # every 10 batches...\n",
    "            \n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 10,\n",
    "                            epoch * len(trainloader) + i)\n",
    "            writer.add_scalar('Learning rate',\n",
    "                            optimizer.param_groups[0]['lr'],\n",
    "                            epoch * len(trainloader) + i)\n",
    "            \n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "#             writer.add_figure('predictions vs. actuals',\n",
    "#                             plot_classes_preds(my_nn, inputs, labels, trainset.classes),\n",
    "#                             global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for k, data in enumerate(testloader, 0):\n",
    "#                     inputs, labels = [d.to(device) for d in data]\n",
    "#                     outputs = my_nn(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     running_loss += loss.item()\n",
    "#                     if k % 10 == 9:\n",
    "#                         # ...log the test loss\n",
    "#                         writer.add_scalar('test loss',\n",
    "#                                         running_loss,\n",
    "#                                         epoch * len(testloader) + k)\n",
    "\n",
    "#                         # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "#     #                     # random mini-batch\n",
    "#     #                     writer.add_figure('predictions vs. actuals',\n",
    "#     #                                     plot_classes_preds(my_nn, inputs, labels, testset.classes),\n",
    "#     #                                     global_step=epoch * len(testloader) + i)\n",
    "#                     running_loss = 0.0\n",
    "\n",
    "\n",
    "end_t = default_timer()\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "writer = SummaryWriter()\n",
    "running_loss = 0.0\n",
    "my_nn = Net()\n",
    "my_nn.to(device)\n",
    "optimizer = optim.Adam(my_nn.parameters(), lr=0.001,)   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "milestones = [k for k in range(0, num_epochs*len(trainloader), 50)]\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.95)\n",
    "\n",
    "start_t = default_timer()\n",
    "\n",
    "batch_tt = np.empty(shape=(10,))\n",
    "k=0\n",
    "for epoch in range(num_epochs-1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        batch_st = default_timer() \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = [d.to(device) for d in data]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = my_nn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        batch_et = default_timer()\n",
    "        \n",
    "        batch_tt[k] = batch_et - batch_st\n",
    "        \n",
    "        k+=1\n",
    "        \n",
    "        if i % 10 == 9:    # every 10 batches...\n",
    "            \n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 10,\n",
    "                            epoch * len(trainloader) + i)\n",
    "            writer.add_scalar('Learning rate',+\n",
    "                            optimizer.param_groups[0]['lr'],\n",
    "                            epoch * len(trainloader) + i)\n",
    "            writer.add_scalar('Average batch time',\n",
    "                            np.mean(batch_tt).item(),\n",
    "                            epoch * len(trainloader) + i)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            batch_tt = np.empty(shape=(10,))\n",
    "            k=0\n",
    "        \n",
    "\n",
    "\n",
    "end_t = default_timer()\n",
    "\n",
    "total_t = end_t - start_t\n",
    "\n",
    "writer.add_scalar('Total training time',\n",
    "                   total_t,\n",
    "                epoch * len(trainloader) + i)\n",
    "\n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199882"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in my_nn.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_t = default_timer()\n",
    "\n",
    "for _ in range(1000):\n",
    "    continue\n",
    "    \n",
    "end_t = default_timer()\n",
    "\n",
    "total_t = end_t - start_t\n",
    "total_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.199999890872277e-05"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-evo-nn]",
   "language": "python",
   "name": "conda-env-torch-evo-nn-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}